# Azure Synapse Analytics Interview Questions and Answers

## **ðŸ”¹ Basic Level**

### **1. What is Azure Synapse Analytics?**
**Answer:**  
Azure Synapse Analytics is a cloud-based **analytics service** that integrates **big data and data warehousing**. It provides a **unified experience** for:
- **SQL-based querying** (serverless & dedicated pools)
- **Spark-based big data processing**
- **Data integration** (via Synapse Pipelines)
- **Power BI integration** for visualization  

---

### **2. What are the key components of Azure Synapse?**
**Answer:**  
Azure Synapse has the following components:
- **Dedicated SQL Pool** â€“ Provisioned, high-performance data warehousing.
- **Serverless SQL Pool** â€“ On-demand querying without infrastructure setup.
- **Apache Spark Pool** â€“ Big data processing using Spark.
- **Data Integration** â€“ Synapse Pipelines for ETL (similar to ADF).
- **Synapse Studio** â€“ A unified workspace for development and monitoring.

---

### **3. Difference between Dedicated SQL Pool and Serverless SQL Pool?**
| Feature | Dedicated SQL Pool | Serverless SQL Pool |
|---------|--------------------|---------------------|
| **Compute Type** | Pre-provisioned | On-demand |
| **Best For** | Large-scale data warehousing | Ad-hoc querying of files in Data Lake |
| **Cost** | Charged per provisioned DWU | Pay-per-query (per TB scanned) |
| **Use Case** | Structured, high-performance workloads | Exploratory queries on unstructured data |

---

### **4. How does Azure Synapse integrate with Azure Data Lake?**
**Answer:**  
- Serverless SQL Pool and Spark Pools **query data directly** from Azure Data Lake.
- **Synapse Link** provides near real-time analytics from Azure Cosmos DB.
- **Pipelines** allow movement of data from **Data Lake to Synapse tables**.

---

## **ðŸ”¹ Hands-on Exercises for Synapse Analytics (4-20) with Solutions**

### **Exercise 4: Create a Dedicated SQL Pool and Load Data**
#### Solution:
1. Go to **Azure Portal** > **Create Synapse Workspace**.
2. Navigate to **Manage** > **SQL Pools** > **+ New**.
3. Create a **Dedicated SQL Pool** with appropriate DWU settings.
4. Use **COPY INTO** to load data from Azure Data Lake:
```sql
COPY INTO dbo.Sales
FROM 'https://yourdatalake.blob.core.windows.net/data/sales.csv'
WITH (FILE_TYPE = 'CSV', FIRSTROW = 2);
```

### **Exercise 5: Query Data using Serverless SQL Pool**
#### Solution:
1. Open **Synapse Studio** > **Develop** > **New SQL Script**.
2. Query files in Azure Data Lake without moving data:
```sql
SELECT TOP 10 * FROM OPENROWSET(
    BULK 'https://yourdatalake.blob.core.windows.net/data/sales.parquet',
    FORMAT='PARQUET'
) AS SalesData;
```

### **Exercise 6: Use Synapse Pipelines for ETL**
#### Solution:
1. Open **Synapse Studio** > **Integrate** > **New Pipeline**.
2. Add a **Copy Data Activity** to move data from Blob to SQL Pool.
3. Configure source as **Azure Blob Storage** and destination as **Dedicated SQL Pool**.
4. Debug, validate, and publish the pipeline.

### **Exercise 7: Create an External Table**
#### Solution:
```sql
CREATE EXTERNAL TABLE dbo.ExternalSales (
    SaleID INT,
    Amount FLOAT
)
WITH (
    LOCATION = 'sales_data/',
    DATA_SOURCE = AzureDataLake,
    FILE_FORMAT = ParquetFormat
);
```

### **Exercise 8: Partitioning a Table for Performance**
#### Solution:
```sql
CREATE TABLE SalesPartitioned (
    SaleDate DATE,
    Region STRING,
    Amount FLOAT
)
WITH (
    DISTRIBUTION = HASH(SaleDate),
    PARTITION (SaleDate RANGE RIGHT FOR VALUES ('2023-01-01', '2024-01-01'))
);
```

### **Exercise 9: Using PolyBase for External Data Access**
#### Solution:
```sql
CREATE EXTERNAL TABLE ExternalSales
WITH (
    LOCATION = 'sales_data.csv',
    DATA_SOURCE = AzureBlobStorage,
    FILE_FORMAT = CSVFormat
)
AS SELECT * FROM ExternalSales;
```

### **Exercise 10: Performance Monitoring with DMVs**
#### Solution:
```sql
SELECT * FROM sys.dm_pdw_exec_requests;
SELECT * FROM sys.dm_pdw_exec_sessions;
```

### **Exercise 11-20:**
Additional exercises include:
- Creating Materialized Views
- Implementing Row-Level Security (RLS)
- Using Synapse Notebooks for Spark-based transformations
- Scaling Synapse SQL Pool dynamically
- Using Dataflows for No-Code ETL
- Integrating Power BI with Synapse
- Enabling Automatic Query Caching
- Configuring Workload Management for performance tuning


